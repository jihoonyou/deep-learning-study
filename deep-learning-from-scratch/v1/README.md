# Deep Learning from Scratch

## Ch.2 퍼셉트론(Perceptron)
- 퍼셉트론: 다수의 신호를 입력으로 받아 하나의 신호를 출력
- 학습: 적절한 매겨변수 값을 정하는 작업
    - 사람은 퍼셉트론의 구조(모델)을 고민하고 컴퓨터에 학습할 데이터를 주는 일을 합니다
- 입력신호에 가중치를 곱한 값과 편향의 합
- 뉴런이 활성화한다: 정해진 한계를 넘어설 때 1을 출력
    - 한계: 임계값 - Theta
- weight(가중치): 각 입력 신호가 결과에 주는 영햑력을 조절하는 매개변수
- bias(편향): 뉴런이 얼마나 쉽게 활성화(결과로 1을 출력)하느냐를 조정하는 매개변수
- 예시의 AND, NAND, OR은 모두 같은 구조의 perceptron, 차이는 가중치 매개변수(w,b)의 값
- 단층 퍼셉트론으로는 **선형** 영역만 표현가능
- 다층 퍼셉트론(multi-layer perceptron)으로 **비선형** 영역 표현 가능
    - 2층 구종의 퍼셉트론으로 XOR 구현가능
    - 즉, 퍼셉트론은 층을 쌓아 더 다양한 것을 표현 가능
- 실제 컴퓨터는 NAND게이트의 조합으로 재현 가능 -> 퍼셉츠론으로 컴퓨터를 표현할 수 있다
    - 이론적으로 2층 퍼셉트론이면 컴퓨터를 만들 수 있다

## Ch.3 신경망(Neural Network)
> 신경망을 통해 가중치 매개변수의 적절합 값을 데이터로부터 자동으로 학습하는 능력

- 입력층, 출력층, 은닉층
    - 사람 눈에 보이지 않기 때문에 은닉층
- 활성화 함수(activation function): 입력 신호의 총합을 출력 신호로 변환하는 함수
    - 가중치가 곱해진 입력 신호의 총합을 계산하고, 그 합을 활성화 함수에 입력해 결과를 나타내는 것
    - 임계값을 경계로 출력이 바뀜
- 퍼셉트론과 신경망의 주된 차이는 활서화 함수
    - 퍼셉트론은 계단함수
    - 신경망은 시그모이드함수 외 다른 활성화 함수
        - 시그모이드란 S자모양이란 뜻
        - `1 / (1 + exp(-x))`
- 계단함수 vs 시그모이드
    - 0을 경계로 출력이 갑자기 변화 vs 연속적으로 변화
    - 0 or 1 vs 실수
        - 즉 퍼셉트론에서는 0,1이 흐름
        - 신경망에서는 영속적인 실수가 흐름
    - 둘 다 입력이 작을 때의 출력은 0에 가깝고, 입력이 커지면 출력이 1에 가까워지는 구조
        - 입력이 중요하면 큰 값을, 입력이 중요하지 않으면 작은 값을 출력
    - 둘 다 비선형함수(직선 한개로 그릴 수 없는 함수)
- 비선형함수
    - 사용이유: 선형 함수를 이용하면 신경망의 층을 깊게 하는 의미가 없어지기 때문
        - 선형함수로 깊게 쌓아도 은닉층이 없는 네틍워크로도 똑같은 기능을 할 수 있다
        - h(x) = cx, h(h(h(x))) = c^3*x 
            - 이 것은 은닉층이 없는 네트워크로 표현 가능
        - 그러므로, 층을 쌓는 혜택을 얻으려면 활성화 함수로 반드시 비선형 함수를 사용해야함
- ReLU 함수: 0을 넘으며녀 그 입력을 그대로 출력하고, 0이하면 0을 출력하는 함수
    - rectified
- 신경망에서의 계산을 행렬 계산으로 정리할 수 있음!
- 3층 신경망 예시
    - a1,a2,a3 = 입력층의 계산값
    - h() - 활성화함수(sigmoid) 호출
    - B1 = 편향값
    - Z1 = h(z1,z2,z3 + B1) = 결과값

    - z1,z2,z3 = 은닉층의 입력값
    - h()
    - B2 = 편향값
    - Z2 = h(z1,z2,z3 + B2) = 결과값
    
    - Z2 = 은닉층의 입력값
    - sigma() - identity 함수(출력층의 활성화 함수는 sigma로 명시)
    - B3 = 편향값
    - Y = sigma(Z2 + B3) = 결과값
- 출력층의 활성화 함수는 풀고자 하는 문제의 성질에 맞게 정합니다
    - 회귀에는 항등함수
    - 2클래스 분류에는 시그모이드 함수
    - 다중 클래스 분류에는 소프트맥스 함수
- 소프트맥스(softmax)
    - 구현시 주의사항:지수 함수 특성상 큰 값을 내뱉으므로 overflow발생 가능
        - 소프트맥스의 지수 함수를 계산할 때 어떤 정수를 더해도 결과는 바뀌지 않는다
        - overflow를 방지하기 위해 입력 신호 중 최댓값을 이용하는 것이 일반적
    - 소프트맥스 함수의 출력은 0에서 1.0사이의 실수
    - 소프트맥수 함수 출력의 총합은 1
        - 소프트맥스 함수를 이용함으로써 문제를 확률적으로 대응할 수 있게 된 것
    - 소프트맥스 함수를 적용해도 대소 관계가 변하지 않으므로, 현업에서 신경망으로 분류할 때 출력층의 소프트맥스 함수를 생략하는게 일반적
- 출력층의 뉴런 수: 풀려는 문제에 맞게 설정(분류하고 싶은 클래스 수로 설정하는 것이 일반적)
    - ex) 0~10 분류 => 출력층의 뉴런 10개
- 추론과정: 순전파(forward propagation)
- MNIST: 손글씨 숫자 이미지 집합
    - 데이터셋 0~9까지의 숫자 이미지
    - 훈련이미지/시험이미지: 60000/10000
    - 28x28 회색조 이미지(1채널)
    - normalize - 0.0~1.0 사이의 값으로 정규화
    - flatten - 입력이미지를 1차원 배열로 (1*28*28 => 784)
    - one_hot_encoding - 정답을 뜻하는 원소만 1로 나머지는 0인 배열 생성
- 코딩예시
    - 입력층 784개인 이유: 28*28=784 (이미지 크기)
    - 출력층 2개인 이유: 0~9개까지의 숫자
    - 은닉층 50개,100개인 이유: 임의로 정한 값
- 정규화: 데이터를 특정 범위로 변환하는 처리
- 전처리: 신경망의 입력 데이터에 특정 변환을 가하는 것

## Ch.4 신경망 학습
> 학습: 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득 하는 것
> 손실함수: 신경망이 학습할 수 있도록 해주는 지표
- 숫자를 인식하는 알고리즘을 밑바닥두터 설계하는 대신, 이미지에서 특징을 추출하고 그특징의 패턴을 기계학습 기술로 학습하는 방법
    - 특징: 입력데이터(입력 이미지)에서 본질적인 데이터(중요한 데이터)를 정확하게 추출할 수 있도록 설계된 변환기
- 훈련데이터와 시ㅣ험 데이터로 나눠 학습과 실험을 수행
    - overfitting 방지: 한 데이터셋에만 지나치게 최적화된 상태
- 손실 함수(loss function): 신경망에서 '하나의 지표'를 기준으로 최적의 매개변수 값을 탐색
    - 일반적으로 평균 제곱 오차와 교차 엔트로피 오차를 사용
    - 손실 함수 쪽 출력이 작으면 정땁 레이블과의 오차도 작은 것
- 미니배치 학습: 신경망 학습에서 훈련 데이터로부터 일부만 골라 학습 하는 것
- 손실함수 사용 이유: 신경망 학습에서 최적의 매개변수(가중치와 편향)를 탐색할 때 손실함수의 값을 가능한 한 작게 하는 매개변수 값을 찾습니다
    - 미분을 계산하고, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복
    - 정확도를 지표로하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문
        - 정확도는 매개변수의 미소한 변화에는 거의 반응을 보이지 않고 반응이 있더라도 그 값이 불연속적으로 갑자기 변함
- 미분함수 - 반올림의 오차 문제를 고려하여 h값 선정(`h=1e-4`)
    - 진정한 미분값과의 오차가 있음을 인지할 것(구현은 차분으로 하기 때문)
- 해석미분: 우리가 아는 그 미분
- 수치미분: 이를 근사치로 계산
- 편미분의 경우 나머지 변수를 상수로 변환하여 풀이가능(즉 변수가 하나인 함수로 정의 후 풀의)
- 기울기(gradient): 모든 변수의 편미분을 벡터로 정리한 것
    - 기울기가 가리키는 쪽은 장소에서 함수의 출력 값을 가장 크게 줄이는 방향
    - 주의: 각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표가 기울기라는 것 (기울기가 가리키는 곳에 정말 함수의 최솟값이 있는지는 미지수)
- 경사법: 함수의 값을 점차 줄이는 것
    - 경사하강법, 경사상승법
- hyper parameter: 학습률 같은 매개변수(사람이 직접설정 필요 - learning rate)
- 신경망 학습에서도 기울기를 구해야함
    - 가중치 매개변수에 대한 손실 함수의 기울기
- 학습 알고리즘
    1. 미니배치
    2. 기울기 산출
    3. 매개변수 갱신
    4. 1~3 반복
- 확률적 강사 하강법(stochastic gradient descent): 데이터를 미니배치로 무작위로 선정하기 때문
- 에폭(epoch): 하나의 단위, 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당
- 신경망 학습에서 훈련 데이터 외의 데이터를 올바르게 인식하는지 확인 필요
    - 오버피팅 방지
        - early stopping

## Ch.5 오차역전파법
> 수치미분은 단순하고 구현하기도 쉽지만 계산 시간이 오래 걸린다는게 단점
- 해당 값이 오르면 최종 값에 어떤 영향을 끼치는지
- 덧셈 노드의 역전파
    - 상류의 값에 1을 곱하기만 할 뿐 그대로 전달
- 곱셈 노드의 역전파
    - 상류의 값에 순전파 때의 입력 신호들을 '서로 바꾼 값'을 곱해서 하류로 전달
- Affine transformation
- 소프트맥스 계층은 추론할 때는 사용하지 않고 학습할 때만 사용!
- 신경망의 계층을 OrderedDict에 보관하는 점이 중요

## Ch.6 학습 관련 기술들
> 손실함수의 값을 간으한 한 낮추는 매개변수를 찾는 것 -> 매개변수의 최적값을 찾는 문제: 최적화
- x^2/20 + y^2 (예시)
- SGD(확률적 경사 하강법)
    - 단점: 탐색 경로가 비효율적(무작정 기울어진 방향으로 진행)
- Momentum
    - x축의 힘은 아주 작지만 방향은 변하지 않아서 한 방향으로 일정하게 가속
    - 전체적으로 SGD보다 x축 방향으로 빠르게 지그재그 움직임이 줄어듭니다
- AdaGrad
    - 학습률을 서서히 낮추는 것
    - 원소중에서 많이(크게 갱신된) 원소는 학습률이 낮아짐
- Adam
    - AdaGrad + Momentum

- graient vaninshing
- ReLU 사용할 때는 He 초기값
- sigmoid, tanh 사용시에는 Xavier 초기값
- batch normalization: 각 층에서 활성화값 분포가 적당히 퍼지도록 설정
    1. 학습을 빨리 진행할 수 있다(학습속도개선)
    2. 초깃값에 크게 의존하지 않는다
    3. 오버피팅을 억제한다
- 오버피팅: 훈련 데이터에 지나치게 적응
    1. 매개변수가 많고 표현력이 높은 모델
    2. 훈련 데이터가 적음
    - 해결법: 가중치 감소, 드롭아웃, 앙상블
- 하이퍼파라미터 최적화

## Ch.7 합섬곱 신경망(CNN)
- 지금까지 본 신경망은 인접하는 계층의 모든 뉴런과 결합되어 있었다
    - fully-conencted
    - Affine-LeRu-Affine-LeRu-Affine-Softmax
- CNN에서는 Convolutional layer와 pooling layer가 추가됨
    - Conv-ReLU-(Pooling)
    - 마지막 출력층에서는 Affine-Softmax 조합을 그대로 사용
- 완전연결 게층의 문제점
    - 데이터의 형상이 무시된다는 것
    - 완전연결 계층에 입력할 때는 3차원의 데이터를 평평한 1차원 데이터로 평탄화해줘야 한다
        - MNIST (1,28,28) =>  (1,784)
    - 공간적 정보: 공간적으로 가까운 픽셀은 값이 비슷하거나.. 거리가 먼 픽셀끼리는 별 연관이 없거나
- 합성곱 계층은 형상을 유지함
    - 3차원 데이터로 입력을 받으며, 다음 계층에도 3차원 데이터로 전달
- 합성곱의 연산 필터 연산(필터-커널)
- 완전연결 신경망에는 가중치 매개변수와 편향이 존재하는데, CNN에서는 필터의 매개변수가 그동안의 '가중치'에 해당(편향도 존재 - 항상 1 by 1 하나만 존재)
- 패딩: 입력데이터 주위에 0을 채운다
    - 출력 크기를 조정할 목적으로 사용
    - 합성곱 연산을 거칠때마다 크기가 작아지는 것을 방지
- 스트라이드: 필터를 적용하는 위치의 간격
- 출력크기 (입력: (H,W), 출력크기: (OH, OW), 패딩: P, 스트라이드: S)
    - OH = (H + 2P - FH)/ S + 1
    - OW = (W + 2P - FW)/ S + 1
    - 주의: 정수로 떨어지는 값이어야 함
        - 정수 값으로 딱 나눠지지 않을 경우, 에러 발생 또는 가장 가까운 정수로 반올림
- 3차원의 경우, 채널방향으로 feature map이 늘어남
    - 입력 데이터의 채널 수와, 필터의 채널 수가 같아야 한다
    - 출력은 한 장의 feature map(채널이 1개인 feature map)
        - 합성곱 연산의 출력으로 다수의 채널을 보내려면, 필터를 다수 사용하는 것
        - (C, H, W)입력데이터 * (FN, C, FH, FW)필터 -> (FN, OH, OW) + (FN, 1, 1)편향 => (FN, OH, OW)출력데이터
- 배치처리 예시
    - (N, C, H, W) * (FN, C, FH, FW) => (N, FN, OH, OW) + (FN, 1, 1) => (N, FN, OH, OW)
- 풀링: 가로/세로 방향의 공간을 줄이는 연산
    - 풀리의 윈도우 크기와 스트라이드는 같은 값으로 설정하는 것이 보통
    - average pooling, max pooling
    - 특징: 학습해야 할 매개변수가 없다, 챈러 수가 변하지 않는다, 입력의 변화에 영향을 적게 받는다
- img2col: iamge to column
    - 합성곱 연산을 좀 더 빠르게
    - 입력 데이터(이미지)를 column으로, 필터를 1열로 전개하여 두 행렬곱으로 계산
    - 2차원의 출력 데이터 결과값을 다시 4차원으로 변형

- 층 깊이에 따른 추출 정보 변화
    - 1번째층: 에지와 블롭
    - 3번째층: 텍스처
    - 5번째층: 사물의 일부
    - 마지막층: 사물의 클래스
    - 층이 깊어지면서 뉴런이 반응하는 대상이 단순한 모양에서 고급 정보로 변화 -> 사물의 의미를 이해하도록 변화

## Ch.8 딥러닝
- data agumentation
- 층을 깊게 하는 것?
    - 대회에서 층의 깊이에 비례해 정확도가 좋아지고 있음
    - 상대적으로 매개변수가 줄어듦
        - 적은 매개변수로 같은 수준의 표현력을 달성가능
    - 작은 필터를 겹쳐 신경망을 깊게 할 때의 장점은 매개변수 수를 줄여 넓은 수용 영역(Receptive Field)을 소화하는데 있음
    - 예) 5 by 5 = 25 vs. 3 by 3 + 3 by 3 = 18
- GoogLeNet
    - 1 by 1 합성곱 연산을 사용하여 채널 쪽 크기를 줄임
- GPU를 사용한 고속화
    - 병렬 수치 연산 고속 처리 가능
    - 딥러닝에서는 대량의 단일-곱셉-누산(또는 큰 행렬의 곱)을 수행해야함
- 분산학습
- 비트 정밀도 감소
- FCN
    - 합성곱 계층만으로 구성된 네트워크
    - 마지막에 공간 크기를 확대하는 처리를 도입