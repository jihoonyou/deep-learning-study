# Deep Learning from Scratch

## Ch.2 퍼셉트론(Perceptron)
- 퍼셉트론: 다수의 신호를 입력으로 받아 하나의 신호를 출력
- 학습: 적절한 매겨변수 값을 정하는 작업
    - 사람은 퍼셉트론의 구조(모델)을 고민하고 컴퓨터에 학습할 데이터를 주는 일을 합니다
- 입력신호에 가중치를 곱한 값과 편향의 합
- 뉴런이 활성화한다: 정해진 한계를 넘어설 때 1을 출력
    - 한계: 임계값 - Theta
- weight(가중치): 각 입력 신호가 결과에 주는 영햑력을 조절하는 매개변수
- bias(편향): 뉴런이 얼마나 쉽게 활성화(결과로 1을 출력)하느냐를 조정하는 매개변수
- 예시의 AND, NAND, OR은 모두 같은 구조의 perceptron, 차이는 가중치 매개변수(w,b)의 값
- 단층 퍼셉트론으로는 **선형** 영역만 표현가능
- 다층 퍼셉트론(multi-layer perceptron)으로 **비선형** 영역 표현 가능
    - 2층 구종의 퍼셉트론으로 XOR 구현가능
    - 즉, 퍼셉트론은 층을 쌓아 더 다양한 것을 표현 가능
- 실제 컴퓨터는 NAND게이트의 조합으로 재현 가능 -> 퍼셉츠론으로 컴퓨터를 표현할 수 있다
    - 이론적으로 2층 퍼셉트론이면 컴퓨터를 만들 수 있다

## Ch.3 신경망(Neural Network)
> 신경망을 통해 가중치 매개변수의 적절합 값을 데이터로부터 자동으로 학습하는 능력

- 입력층, 출력층, 은닉층
    - 사람 눈에 보이지 않기 때문에 은닉층
- 활성화 함수(activation function): 입력 신호의 총합을 출력 신호로 변환하는 함수
    - 가중치가 곱해진 입력 신호의 총합을 계산하고, 그 합을 활성화 함수에 입력해 결과를 나타내는 것
    - 임계값을 경계로 출력이 바뀜
- 퍼셉트론과 신경망의 주된 차이는 활서화 함수
    - 퍼셉트론은 계단함수
    - 신경망은 시그모이드함수 외 다른 활성화 함수
        - 시그모이드란 S자모양이란 뜻
        - `1 / (1 + exp(-x))`
- 계단함수 vs 시그모이드
    - 0을 경계로 출력이 갑자기 변화 vs 연속적으로 변화
    - 0 or 1 vs 실수
        - 즉 퍼셉트론에서는 0,1이 흐름
        - 신경망에서는 영속적인 실수가 흐름
    - 둘 다 입력이 작을 때의 출력은 0에 가깝고, 입력이 커지면 출력이 1에 가까워지는 구조
        - 입력이 중요하면 큰 값을, 입력이 중요하지 않으면 작은 값을 출력
    - 둘 다 비선형함수(직선 한개로 그릴 수 없는 함수)
- 비선형함수
    - 사용이유: 선형 함수를 이용하면 신경망의 층을 깊게 하는 의미가 없어지기 때문
        - 선형함수로 깊게 쌓아도 은닉층이 없는 네틍워크로도 똑같은 기능을 할 수 있다
        - h(x) = cx, h(h(h(x))) = c^3*x 
            - 이 것은 은닉층이 없는 네트워크로 표현 가능
        - 그러므로, 층을 쌓는 혜택을 얻으려면 활성화 함수로 반드시 비선형 함수를 사용해야함
- ReLU 함수: 0을 넘으며녀 그 입력을 그대로 출력하고, 0이하면 0을 출력하는 함수
    - rectified
- 신경망에서의 계산을 행렬 계산으로 정리할 수 있음!
- 3층 신경망 예시
    - a1,a2,a3 = 입력층의 계산값
    - h() - 활성화함수(sigmoid) 호출
    - B1 = 편향값
    - Z1 = h(z1,z2,z3 + B1) = 결과값

    - z1,z2,z3 = 은닉층의 입력값
    - h()
    - B2 = 편향값
    - Z2 = h(z1,z2,z3 + B2) = 결과값
    
    - Z2 = 은닉층의 입력값
    - sigma() - identity 함수(출력층의 활성화 함수는 sigma로 명시)
    - B3 = 편향값
    - Y = sigma(Z2 + B3) = 결과값
- 출력층의 활성화 함수는 풀고자 하는 문제의 성질에 맞게 정합니다
    - 회귀에는 항등함수
    - 2클래스 분류에는 시그모이드 함수
    - 다중 클래스 분류에는 소프트맥스 함수
- 소프트맥스(softmax)
    - 구현시 주의사항:지수 함수 특성상 큰 값을 내뱉으므로 overflow발생 가능
        - 소프트맥스의 지수 함수를 계산할 때 어떤 정수를 더해도 결과는 바뀌지 않는다
        - overflow를 방지하기 위해 입력 신호 중 최댓값을 이용하는 것이 일반적
    - 소프트맥스 함수의 출력은 0에서 1.0사이의 실수
    - 소프트맥수 함수 출력의 총합은 1
        - 소프트맥스 함수를 이용함으로써 문제를 확률적으로 대응할 수 있게 된 것
    - 소프트맥스 함수를 적용해도 대소 관계가 변하지 않으므로, 현업에서 신경망으로 분류할 때 출력층의 소프트맥스 함수를 생략하는게 일반적
- 출력층의 뉴런 수: 풀려는 문제에 맞게 설정(분류하고 싶은 클래스 수로 설정하는 것이 일반적)
    - ex) 0~10 분류 => 출력층의 뉴런 10개
- 추론과정: 순전파(forward propagation)
- MNIST: 손글씨 숫자 이미지 집합
    - 데이터셋 0~9까지의 숫자 이미지
    - 훈련이미지/시험이미지: 60000/10000
    - 28x28 회색조 이미지(1채널)
    - normalize - 0.0~1.0 사이의 값으로 정규화
    - flatten - 입력이미지를 1차원 배열로 (1*28*28 => 784)
    - one_hot_encoding - 정답을 뜻하는 원소만 1로 나머지는 0인 배열 생성
- 코딩예시
    - 입력층 784개인 이유: 28*28=784 (이미지 크기)
    - 출력층 2개인 이유: 0~9개까지의 숫자
    - 은닉층 50개,100개인 이유: 임의로 정한 값
- 정규화: 데이터를 특정 범위로 변환하는 처리
- 전처리: 신경망의 입력 데이터에 특정 변환을 가하는 것

## Ch.4 신경망 학습
> 학습: 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득 하는 것
> 손실함수: 신경망이 학습할 수 있도록 해주는 지표
- 숫자를 인식하는 알고리즘을 밑바닥두터 설계하는 대신, 이미지에서 특징을 추출하고 그특징의 패턴을 기계학습 기술로 학습하는 방법
    - 특징: 입력데이터(입력 이미지)에서 본질적인 데이터(중요한 데이터)를 정확하게 추출할 수 있도록 설계된 변환기
- 훈련데이터와 시ㅣ험 데이터로 나눠 학습과 실험을 수행
    - overfitting 방지: 한 데이터셋에만 지나치게 최적화된 상태
- 손실 함수(loss function): 신경망에서 '하나의 지표'를 기준으로 최적의 매개변수 값을 탐색
    - 일반적으로 평균 제곱 오차와 교차 엔트로피 오차를 사용
    - 손실 함수 쪽 출력이 작으면 정땁 레이블과의 오차도 작은 것
- 미니배치 학습: 신경망 학습에서 훈련 데이터로부터 일부만 골라 학습 하는 것
- 손실함수 사용 이유: 신경망 학습에서 최적의 매개변수(가중치와 편향)를 탐색할 때 손실함수의 값을 가능한 한 작게 하는 매개변수 값을 찾습니다
    - 미분을 계산하고, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복
    - 정확도를 지표로하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문
        - 정확도는 매개변수의 미소한 변화에는 거의 반응을 보이지 않고 반응이 있더라도 그 값이 불연속적으로 갑자기 변함
- 미분함수 - 반올림의 오차 문제를 고려하여 h값 선정(`h=1e-4`)
    - 진정한 미분값과의 오차가 있음을 인지할 것(구현은 차분으로 하기 때문)
- 해석미분: 우리가 아는 그 미분
- 수치미분: 이를 근사치로 계산
- 편미분의 경우 나머지 변수를 상수로 변환하여 풀이가능(즉 변수가 하나인 함수로 정의 후 풀의)
- 기울기(gradient): 모든 변수의 편미분을 벡터로 정리한 것
    - 기울기가 가리키는 쪽은 장소에서 함수의 출력 값을 가장 크게 줄이는 방향
    - 주의: 각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표가 기울기라는 것 (기울기가 가리키는 곳에 정말 함수의 최솟값이 있는지는 미지수)
- 경사법: 함수의 값을 점차 줄이는 것
    - 경사하강법, 경사상승법
- hyper parameter: 학습률 같은 매개변수(사람이 직접설정 필요 - learning rate)
- 신경망 학습에서도 기울기를 구해야함
    - 가중치 매개변수에 대한 손실 함수의 기울기
- 학습 알고리즘
    1. 미니배치
    2. 기울기 산출
    3. 매개변수 갱신
    4. 1~3 반복
- 확률적 강사 하강법(stochastic gradient descent): 데이터를 미니배치로 무작위로 선정하기 때문
- 에폭(epoch): 하나의 단위, 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당
- 신경망 학습에서 훈련 데이터 외의 데이터를 올바르게 인식하는지 확인 필요
    - 오버피팅 방지
        - early stopping

## Ch.5 오차역전파법
> 수치미분은 단순하고 구현하기도 쉽지만 계산 시간이 오래 걸린다는게 단점
- 해당 값이 오르면 최종 값에 어떤 영향을 끼치는지
- 덧셈 노드의 역전파
    - 상류의 값에 1을 곱하기만 할 뿐 그대로 전달
- 곱셈 노드의 역전파
    - 상류의 값에 순전파 때의 입력 신호들을 '서로 바꾼 값'을 곱해서 하류로 전달
- Affine transformation
- 소프트맥스 계층은 추론할 때는 사용하지 않고 학습할 때만 사용!
- 신경망의 계층을 OrderedDict에 보관하는 점이 중요
