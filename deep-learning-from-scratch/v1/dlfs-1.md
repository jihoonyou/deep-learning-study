# Deep Learning from Scratch

## Ch.2 퍼셉트론(Perceptron)
- 퍼셉트론: 다수의 신호를 입력으로 받아 하나의 신호를 출력
- 학습: 적절한 매겨변수 값을 정하는 작업
    - 사람은 퍼셉트론의 구조(모델)을 고민하고 컴퓨터에 학습할 데이터를 주는 일을 합니다
- 입력신호에 가중치를 곱한 값과 편향의 합
- 뉴런이 활성화한다: 정해진 한계를 넘어설 때 1을 출력
    - 한계: 임계값 - Theta
- weight(가중치): 각 입력 신호가 결과에 주는 영햑력을 조절하는 매개변수
- bias(편향): 뉴런이 얼마나 쉽게 활성화(결과로 1을 출력)하느냐를 조정하는 매개변수
- 예시의 AND, NAND, OR은 모두 같은 구조의 perceptron, 차이는 가중치 매개변수(w,b)의 값
- 단층 퍼셉트론으로는 **선형** 영역만 표현가능
- 다층 퍼셉트론(multi-layer perceptron)으로 **비선형** 영역 표현 가능
    - 2층 구종의 퍼셉트론으로 XOR 구현가능
    - 즉, 퍼셉트론은 층을 쌓아 더 다양한 것을 표현 가능
- 실제 컴퓨터는 NAND게이트의 조합으로 재현 가능 -> 퍼셉츠론으로 컴퓨터를 표현할 수 있다
    - 이론적으로 2층 퍼셉트론이면 컴퓨터를 만들 수 있다

## Ch.3 신경망(Neural Network)
> 신경망을 통해 가중치 매개변수의 적절합 값을 데이터로부터 자동으로 학습하는 능력

- 입력층, 출력층, 은닉층
    - 사람 눈에 보이지 않기 때문에 은닉층
- 활성화 함수(activation function): 입력 신호의 총합을 출력 신호로 변환하는 함수
    - 가중치가 곱해진 입력 신호의 총합을 계산하고, 그 합을 활성화 함수에 입력해 결과를 나타내는 것
    - 임계값을 경계로 출력이 바뀜
- 퍼셉트론과 신경망의 주된 차이는 활서화 함수
    - 퍼셉트론은 계단함수
    - 신경망은 시그모이드함수 외 다른 활성화 함수
        - 시그모이드란 S자모양이란 뜻
        - `1 / (1 + exp(-x))`
- 계단함수 vs 시그모이드
    - 0을 경계로 출력이 갑자기 변화 vs 연속적으로 변화
    - 0 or 1 vs 실수
        - 즉 퍼셉트론에서는 0,1이 흐름
        - 신경망에서는 영속적인 실수가 흐름
    - 둘 다 입력이 작을 때의 출력은 0에 가깝고, 입력이 커지면 출력이 1에 가까워지는 구조
        - 입력이 중요하면 큰 값을, 입력이 중요하지 않으면 작은 값을 출력
    - 둘 다 비선형함수(직선 한개로 그릴 수 없는 함수)
- 비선형함수
    - 사용이유: 선형 함수를 이용하면 신경망의 층을 깊게 하는 의미가 없어지기 때문
        - 선형함수로 깊게 쌓아도 은닉층이 없는 네틍워크로도 똑같은 기능을 할 수 있다
        - h(x) = cx, h(h(h(x))) = c^3*x 
            - 이 것은 은닉층이 없는 네트워크로 표현 가능
        - 그러므로, 층을 쌓는 혜택을 얻으려면 활성화 함수로 반드시 비선형 함수를 사용해야함
- ReLU 함수: 0을 넘으며녀 그 입력을 그대로 출력하고, 0이하면 0을 출력하는 함수
    - rectified
- 신경망에서의 계산을 행렬 계산으로 정리할 수 있음!
- 3층 신경망 예시
    - a1,a2,a3 = 입력층의 계산값
    - h() - 활성화함수(sigmoid) 호출
    - B1 = 편향값
    - Z1 = h(z1,z2,z3 + B1) = 결과값

    - z1,z2,z3 = 은닉층의 입력값
    - h()
    - B2 = 편향값
    - Z2 = h(z1,z2,z3 + B2) = 결과값
    
    - Z2 = 은닉층의 입력값
    - sigma() - identity 함수(출력층의 활성화 함수는 sigma로 명시)
    - B3 = 편향값
    - Y = sigma(Z2 + B3) = 결과값
- 출력층의 활성화 함수는 풀고자 하는 문제의 성질에 맞게 정합니다
    - 회귀에는 항등함수
    - 2클래스 분류에는 시그모이드 함수
    - 다중 클래스 분류에는 소프트맥스 함수
- 소프트맥스(softmax)
    - 구현시 주의사항:지수 함수 특성상 큰 값을 내뱉으므로 overflow발생 가능
        - 소프트맥스의 지수 함수를 계산할 때 어떤 정수를 더해도 결과는 바뀌지 않는다
        - overflow를 방지하기 위해 입력 신호 중 최댓값을 이용하는 것이 일반적
    - 소프트맥스 함수의 출력은 0에서 1.0사이의 실수
    - 소프트맥수 함수 출력의 총합은 1
        - 소프트맥스 함수를 이용함으로써 문제를 확률적으로 대응할 수 있게 된 것
    - 소프트맥스 함수를 적용해도 대소 관계가 변하지 않으므로, 현업에서 신경망으로 분류할 때 출력층의 소프트맥스 함수를 생략하는게 일반적
- 출력층의 뉴런 수: 풀려는 문제에 맞게 설정(분류하고 싶은 클래스 수로 설정하는 것이 일반적)
    - ex) 0~10 분류 => 출력층의 뉴런 10개
- 추론과정: 순전파(forward propagation)
- MNIST: 손글씨 숫자 이미지 집합
    - 데이터셋 0~9까지의 숫자 이미지
    - 훈련이미지/시험이미지: 60000/10000
    - 28x28 회색조 이미지(1채널)
    - normalize - 0.0~1.0 사이의 값으로 정규화
    - flatten - 입력이미지를 1차원 배열로 (1*28*28 => 784)
    - one_hot_encoding - 정답을 뜻하는 원소만 1로 나머지는 0인 배열 생성
- 코딩예시
    - 입력층 784개인 이유: 28*28=784 (이미지 크기)
    - 출력층 2개인 이유: 0~9개까지의 숫자
    - 은닉층 50개,100개인 이유: 임의로 정한 값
- 정규화: 데이터를 특정 범위로 변환하는 처리
- 전처리: 신경망의 입력 데이터에 특정 변환을 가하는 것
